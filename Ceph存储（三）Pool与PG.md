# Ceph存储（三）Pool与PG

## 存储池

存储池（Pool）通常情况下可以为特定的应用程序或者不同类型的数据需求创建专用的存储池，如rdb存储池，rgw存储池，个人专用存储池，某部门专用存储池等。

客户端在连接到Ceph集群的时候必须指定一个存储池的名称，并完成用户名和密钥的认证方可连接至指定的存储池。 

### 存储池类型

通过存储池的使用类型有rbd存储池，个人存储池等。按存储池的官方类型可以将存储池分为副本池（replicated）和纠删码池（ erasure code ）

#### 副本池

默认的存储池类型，把每个存入的对象（Object）存储为多个副本，其中分为主副本和从副本，从副本相当于备份副本。如果客户端在上传对象的时候不指定副本数，默认为3个副本。关于副本池数据存储流程如下图所示。

![](http://121.43.168.35/wp-content/uploads/2019/05/2-1.png)

在开始存数据之前会计算出该对象存储的主副本与从副本的位置，首先会将数据存入到主副本，然后主副本再将数据分别同步到从副本。主副本与从副本同步完毕后会通知主副本，这时候主副本再响应客户端，并表示数据上传成功。所以如果客户端收到存储成功的请求后，说明数据已经完成了所有副本的存储。

#### 纠删码池

此类型会将数据存储为K+M，其中K数据块数量。每个对象存储到Ceph集群的时候会分成多个数据块分开进行存储。而M为为编码块，也代表最多容忍可坏的数据块数量。类似于磁盘阵列RAID5，在最大化利用空间的同时，还能保证数据丢失可恢复性，相比副本池更节约磁盘的空间。

因为副本池很浪费存储空间，如果Ceph集群总容量为100T，如果使用副本池，那么实际可用空间按3个副本算，那么只有30T出头。而使用纠删码池就可以更大化的利用空间，但纠删码池更浪费计算资源。

如存储一个100M的资源，如果使用副本池，按3副本计算实际上要使用300M的空间。而使用纠删码池，如果将100M资源分为25块，如果将M指定为2，那么总共只需要108M空间即可，计算公式为100+100/25*2。

注意：如果存储RBD镜像，那么不支持纠删码池。关于此类型存储池使用不多，不做过多介绍。

## PG

PG英文全称 Placement Group，中文称之为归置组。

### PG的作用

PG相当于一个虚拟组件，出于集群伸缩，性能方面的考虑。Ceph将每个存储池分为多个PG，如果存储池为副本池类型，并会给该存储池每个PG分配一个主OSD和多个从OSD，当数据量大的时候PG将均衡的分布行不通集群中的每个OSD上面。

### 如何正确设置PG数量

一个Ceph集群中，PG的数量不能随便的设定。而应该合理的设定。通常如果一个集群有个超过50个OSD，建议每个OSD大约有50到100到PG。如果有更大规模的集群，建议每个OSD大约100到200个PG。

总PG的数量基本计算公式为： (总OSD数*每个OSD计划PG数)/副本数 => 总PG数

因为总PG数推荐为2的N次幂，计算出来的结果不一定为2的N次幂，需要取比计算结果小的，最近的一个数，并且这个数为2的N次幂。因为2的N次幂计算结果最快，这样可以减少CPU内存的消耗。

说明：总的PG数=所有Pool中定义的PG数的总和

假设集群中有50个OSD，按推荐超过50个OSD的RADOS集群推荐的每个OSD的PG数为50到100。计划PG数为60，这时候公式为：(50*60)/3 = 1000 => 512，所以这时候集群中总的PG数量推荐为512个。

为什么我们定义每个OSD的PG数为60，而不是100?

如果一个OSD的PG数越多，那么在移动数据的时候会更少，但更浪费CPU的内存。如果一个OSD的PG数越少，这样移动的数据会越多，会对正常的性能产生负面影响。而在OSD之间进行数据持久存储和数据分布需要较多的PG，它们的数据应该减少到最大性能所需要的最小值，也减少CPU和内存资源。这个性能需要根据自身情况而定。 备注：PG所对应的就是实际的存储对象。移动PG就相当于移动数据。 

### PG的常见状态

PG的状态可以由以下命令获取。
    
    
    ~]$ ceph pg stat

通常状态为active+clean表示正常，还可以使用以下命令查看详细信息。
    
    
    ~]$ ceph pg dump

  * Active：表示主OSD和从OSD都处理就绪状态，可常用提供客户端请求。 
  * Clean：表示主OSD和从OSD都处理就绪状态，所有对象的副本均符合期望。
  * Peering：通常此状态表示正在将主OSD和从OSD的对象同步一致的过程，如果这个过程完成后，通过状态就为Active。
  * Degraded：当我某OSD标记为down的时候，这时候映射到此的OSD的PG将进入Degraded（降级）状态，当OSD重新up，并完成Peering后，将重回正常状态。 一旦标记为down超过5分钟，这时候此OSD将被T出集群，Ceph将启动自恢复操作，相当于重新分配PG，直到状态正常。 有时候某个OSD不可用，崩溃的时候也会处此此状态。
  * Stale：每个OSD都要周期性的向Monitor报千其主OSD所持有的PG最新统计数据。如果因为任何原因某个主OSD主法正常向Monitor报告，或由其它OSD报告某个OSD已经挂了，这时候以以OSD为主的其它OSD都将标记为此状态。
  * Undersized：当PG中的副本数少于其存储池指定的个数的时候，就为此状态。
  * Scrubbing：各OSD还会周期性的检查其持有的数据对象的完性，以确保主和从的数据一致，这时候状态就为此状态。 另外PG偶尔还需要查检确保一个对象的OSD上能按位匹配，这时候状态为scrubbing+deep。
  * Recovering：当添加一个新的OSD到集中中，或者某个OSD宕掉时，PG有要嗵会被重新映射，而这些处理同步过各中的PG则会标记为recovering。
  * Backfilling：新OSD加放到集群后，Ceph会进入数据重新均衡的状态，即一些数据会从现有OSD迁移到新的OSD，这些操作过程即为backfill。
